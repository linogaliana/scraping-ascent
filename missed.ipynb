{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in fs.ls(\"lgaliana/cyclisme/\") if file.endswith(\".parquet\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd \n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "files = [file for file in fs.ls(\"lgaliana/cyclisme/\") if file.endswith(\".parquet\")]\n",
    "\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    with fs.open(file, \"rb\") as f:\n",
    "        df = pd.read_parquet(f)\n",
    "        dataframes.append(df)\n",
    "\n",
    "list_files = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_downloaded = pd.DataFrame(\n",
    "    fs.ls(\"lgaliana/cyclisme/data/geojson/split/\"),\n",
    "    columns = [\"downloaded\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_downloaded['file'] = list_downloaded['downloaded'].str.rsplit(\"/\").str[-1]\n",
    "list_downloaded['file'] = \"https://www.cols-cyclisme.com/gpx/\" + list_downloaded['file']\n",
    "list_downloaded['file'] = list_downloaded['file'].str.replace(\"geojson\", \"gpx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all = list_files.merge(list_downloaded, left_on = \"id\", right_on=\"file\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_downloaded = list_all.loc[list_all['downloaded'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from scraping import extract_info_col, get_gpx_from_url\n",
    "import time\n",
    "\n",
    "details_df = pd.DataFrame()\n",
    "traces = gpd.GeoDataFrame()\n",
    "\n",
    "Path(\"./gpx\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#details_df_old = pd.DataFrame()\n",
    "#traces_old = gpd.GeoDataFrame()\n",
    "details_df = pd.DataFrame()\n",
    "traces = gpd.GeoDataFrame()\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in non_downloaded.iterrows():\n",
    "    if pd.notnull(row['id']): # and row['id'] not in details_df_old['url'].tolist():\n",
    "        print(f\"{index}, {row['id']}\")\n",
    "        col_url = row['href']\n",
    "        col_info_df = extract_info_col(col_url, id=index)\n",
    "        col_info_df['url'] = row['id']\n",
    "        trace = get_gpx_from_url(row['id'])\n",
    "        details_df = pd.concat([details_df, col_info_df], ignore_index=True)\n",
    "        traces = pd.concat([traces, trace])\n",
    "        time.sleep(1)  # Sleep for 1 second between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = non_downloaded.copy()\n",
    "\n",
    "df[\"url\"] = df[\"id\"]\n",
    "df[\"id\"] = df[\"url\"].str.rsplit(\"/\").str[-1].str.replace(\".gpx\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from scraping import create_geojson_from_gpx, get_max_altitude_rows\n",
    "\n",
    "# Create a directory to store the downloaded images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"data/derived/\", exist_ok=True)\n",
    "\n",
    "# ----------------------------------\n",
    "# duplicate images on sspcloud\n",
    "\n",
    "for url in df[\"Profil Image URL\"]:\n",
    "    # Check if the URL is available\n",
    "    if url != \"Not available\":\n",
    "        # Get the filename from the URL\n",
    "        filename = url.split(\"/\")[-1]\n",
    "\n",
    "        # Check if the file already exists locally\n",
    "        if os.path.exists(f\"images/{filename}\"):\n",
    "            print(f\"Image already exists: {filename}\")\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "            # Send a GET request to download the image\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Save the image locally\n",
    "                with open(f\"images/{filename}\", \"wb\") as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"Image downloaded: {filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download image: {filename}\")\n",
    "    else:\n",
    "        print(\"Profil Image URL is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# create geojson for climbing ascent\n",
    "from unidecode import unidecode\n",
    "\n",
    "filename_summits = \"missed.geojson\"\n",
    "\n",
    "geojsons = create_geojson_from_gpx()\n",
    "df_max_alt = get_max_altitude_rows(geojsons)\n",
    "\n",
    "df_max_alt[\"url\"] = \"https://www.cols-cyclisme.com//gpx/\" + df_max_alt[\"url\"]\n",
    "df_max_alt[\"id\"] = df_max_alt[\"url\"].str.rsplit(\"/\").str[-1].str.replace(\".gpx\", \"\")\n",
    "df_max_alt = df_max_alt.drop(\"url\", axis=\"columns\")\n",
    "df_max_alt = df_max_alt.merge(df, on=\"id\")\n",
    "\n",
    "\n",
    "sanitized_columns = (\n",
    "    df_max_alt.columns.map(unidecode)  # Transliterate characters to ASCII\n",
    "    .str.replace(\"%\", \"percent\")  # Replace '%' with 'percent'\n",
    "    .str.lower()  # Convert to lowercase\n",
    "    .str.replace(\n",
    "        r\"[^a-zA-Z0-9_\\s]\", \"\", regex=True\n",
    "    )  # Remove special characters except underscores and spaces\n",
    "    .str.replace(\" \", \"_\")  # Replace spaces with underscores\n",
    ")\n",
    "df_max_alt.columns = sanitized_columns\n",
    "columns_to_sanitize = [\n",
    "    \"altitude\",\n",
    "    \"longueur\",\n",
    "    \"denivellation\",\n",
    "    \"percent_moyen\",\n",
    "    \"percent_maximal\",\n",
    "]\n",
    "df_max_alt[columns_to_sanitize] = (\n",
    "    df_max_alt.loc[:, columns_to_sanitize]\n",
    "    .replace({r\"\\s*\": \"\", \"km\": \"\", \"m\": \"\", \"%\": \"\"}, regex=True)\n",
    "    .astype(float)\n",
    ")\n",
    "if 'vtt' in df_max_alt.columns:\n",
    "    df_max_alt['vtt'] = (\n",
    "        df_max_alt['vtt'] == \"ATTENTION : cette ascension nécéssite l'utilisation d'un VTT\"\n",
    "    )\n",
    "if 'ouverture' in df_max_alt.columns:\n",
    "    df_max_alt = df_max_alt.drop(\"ouverture\", axis=\"columns\")\n",
    "df_max_alt[\"category\"] = pd.cut(\n",
    "    df_max_alt[\"denivellation\"],\n",
    "    right=False,\n",
    "    bins=[80, 160, 320, 640, 800, float(\"inf\")], \n",
    "    labels=[\"Cat 4\", \"Cat 3\", \"Cat 2\", \"Cat 1\", \"HC\"]\n",
    ")\n",
    "df_max_alt = df_max_alt.dropna(subset=[\"category\"])\n",
    "df_max_alt[\"category\"] = df_max_alt[\"category\"].astype(str)\n",
    "df_max_alt = df_max_alt.loc[~df_max_alt['massif'].str.contains('Canada')]\n",
    "df_max_alt = df_max_alt.loc[~df_max_alt['massif'].str.contains('(Réunion, France)')]\n",
    "df_max_alt.to_file(filename_summits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString\n",
    "\n",
    "# Create routes ----------------------------------\n",
    "\n",
    "routes = create_geojson_from_gpx(three_dim=True)\n",
    "\n",
    "# Split the routes geodataframe by 'url' column values\n",
    "split_routes = routes.groupby(\"url\")\n",
    "\n",
    "split_routes = routes.groupby([\"url\"])[\"geometry\"].apply(\n",
    "    lambda x: LineString(x.tolist())\n",
    ")\n",
    "split_routes = gpd.GeoDataFrame(split_routes, geometry=\"geometry\")\n",
    "split_routes = split_routes.groupby(\"url\")\n",
    "\n",
    "# Write each split into a separate .geojson file\n",
    "for value, group in split_routes:\n",
    "    filename = value.replace(\".gpx\", \"\")\n",
    "    file_path = f\"data/derived/{filename}.geojson\"\n",
    "    group.to_file(file_path, driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
